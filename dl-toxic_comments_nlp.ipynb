{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bfffa8e-7940-437b-8755-8691f246f5b4",
   "metadata": {},
   "source": [
    "## NLP\n",
    "\n",
    "Даны токсичные и нетоксичные комментарии. Надо расклассифицировать эти комменты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31ba5ae3-8c01-4734-b1f0-326e81c33825",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:21.752360Z",
     "iopub.status.busy": "2024-06-09T18:38:21.751076Z",
     "iopub.status.idle": "2024-06-09T18:38:23.689501Z",
     "shell.execute_reply": "2024-06-09T18:38:23.688699Z",
     "shell.execute_reply.started": "2024-06-09T18:38:21.752312Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import pymorphy2\n",
    "from pymystem3 import Mystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bcf568a0-315b-4e4d-af72-713551aa7fce",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:23.692117Z",
     "iopub.status.busy": "2024-06-09T18:38:23.690989Z",
     "iopub.status.idle": "2024-06-09T18:38:23.730011Z",
     "shell.execute_reply": "2024-06-09T18:38:23.729302Z",
     "shell.execute_reply.started": "2024-06-09T18:38:23.692060Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "715cbb0c-4c97-4bf4-a77d-72d4cf4b36c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:23.732115Z",
     "iopub.status.busy": "2024-06-09T18:38:23.731074Z",
     "iopub.status.idle": "2024-06-09T18:38:23.741772Z",
     "shell.execute_reply": "2024-06-09T18:38:23.741064Z",
     "shell.execute_reply.started": "2024-06-09T18:38:23.732079Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01869fd6-ad7d-48fb-bf02-68cef4f71528",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:23.744191Z",
     "iopub.status.busy": "2024-06-09T18:38:23.743505Z",
     "iopub.status.idle": "2024-06-09T18:38:23.759122Z",
     "shell.execute_reply": "2024-06-09T18:38:23.758378Z",
     "shell.execute_reply.started": "2024-06-09T18:38:23.744156Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bea47cfe-8a09-4625-8cc8-e612884ad08d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:23.761107Z",
     "iopub.status.busy": "2024-06-09T18:38:23.760172Z",
     "iopub.status.idle": "2024-06-09T18:38:23.772168Z",
     "shell.execute_reply": "2024-06-09T18:38:23.771526Z",
     "shell.execute_reply.started": "2024-06-09T18:38:23.761070Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "\n",
    "# # Укажите путь к вашему JSON-файлу\n",
    "# file_path = '/home/jupyter/datasphere/project/train.json'\n",
    "\n",
    "# try:\n",
    "#     with open(file_path, 'r') as file:\n",
    "#         json_data = \"\"\n",
    "#         for line in file:\n",
    "#             json_data += line.strip()\n",
    "        \n",
    "#         # Поиск всех JSON объектов и их загрузка\n",
    "#         while json_data:\n",
    "#             try:\n",
    "#                 data, idx = json.JSONDecoder().raw_decode(json_data)\n",
    "#                 print(data)\n",
    "#                 json_data = json_data[idx:].strip()\n",
    "#             except json.JSONDecodeError as e:\n",
    "#                 print(\"Ошибка при декодировании JSON:\", e)\n",
    "#                 break\n",
    "# except Exception as e:\n",
    "#     print(\"Произошла ошибка:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f96721f9-bed4-4c05-877b-d323ffe6238a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:23.774153Z",
     "iopub.status.busy": "2024-06-09T18:38:23.773229Z",
     "iopub.status.idle": "2024-06-09T18:38:23.793466Z",
     "shell.execute_reply": "2024-06-09T18:38:23.792638Z",
     "shell.execute_reply.started": "2024-06-09T18:38:23.774113Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from datasets import load_dataset\n",
    "\n",
    "# dataset = load_dataset(\"Hrukanina/ultra-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2a3ddad5-5658-4a8d-97b3-a85d0be4bdc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:23.795553Z",
     "iopub.status.busy": "2024-06-09T18:38:23.794946Z",
     "iopub.status.idle": "2024-06-09T18:38:23.806484Z",
     "shell.execute_reply": "2024-06-09T18:38:23.805849Z",
     "shell.execute_reply.started": "2024-06-09T18:38:23.795519Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install --upgrade fsspec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "103d2372-ce54-471c-b5af-d92d9df51e2a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:23.808609Z",
     "iopub.status.busy": "2024-06-09T18:38:23.807627Z",
     "iopub.status.idle": "2024-06-09T18:38:23.819951Z",
     "shell.execute_reply": "2024-06-09T18:38:23.819225Z",
     "shell.execute_reply.started": "2024-06-09T18:38:23.808572Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d2e960b-9cd4-4897-b52a-c8a1a821b79b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:23.822227Z",
     "iopub.status.busy": "2024-06-09T18:38:23.821Z",
     "iopub.status.idle": "2024-06-09T18:38:23.830059Z",
     "shell.execute_reply": "2024-06-09T18:38:23.829325Z",
     "shell.execute_reply.started": "2024-06-09T18:38:23.822169Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4a15530-172a-4383-a1c1-3d9730479d77",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:23.833193Z",
     "iopub.status.busy": "2024-06-09T18:38:23.832458Z",
     "iopub.status.idle": "2024-06-09T18:38:23.842719Z",
     "shell.execute_reply": "2024-06-09T18:38:23.842047Z",
     "shell.execute_reply.started": "2024-06-09T18:38:23.833167Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "75f8f69b-ce93-4d2f-9422-237127e17605",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:23.844324Z",
     "iopub.status.busy": "2024-06-09T18:38:23.843770Z",
     "iopub.status.idle": "2024-06-09T18:38:23.856106Z",
     "shell.execute_reply": "2024-06-09T18:38:23.855457Z",
     "shell.execute_reply.started": "2024-06-09T18:38:23.844292Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# nltk.download('averaged_perceptron_tagger_ru')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65b74419-e1a0-4208-9fd5-e0517af8e579",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:23.857701Z",
     "iopub.status.busy": "2024-06-09T18:38:23.857193Z",
     "iopub.status.idle": "2024-06-09T18:38:23.866142Z",
     "shell.execute_reply": "2024-06-09T18:38:23.865546Z",
     "shell.execute_reply.started": "2024-06-09T18:38:23.857667Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "770c4792-90cd-4d9c-b1f2-22d7c6ef91d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:23.867476Z",
     "iopub.status.busy": "2024-06-09T18:38:23.867005Z",
     "iopub.status.idle": "2024-06-09T18:38:23.885946Z",
     "shell.execute_reply": "2024-06-09T18:38:23.885269Z",
     "shell.execute_reply.started": "2024-06-09T18:38:23.867443Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install pymorphy3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3132a21d-97a9-464e-9d36-36202457b9ba",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:23.887772Z",
     "iopub.status.busy": "2024-06-09T18:38:23.886909Z",
     "iopub.status.idle": "2024-06-09T18:38:23.902872Z",
     "shell.execute_reply": "2024-06-09T18:38:23.902218Z",
     "shell.execute_reply.started": "2024-06-09T18:38:23.887737Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install pymorphy2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "744e9ec0-d1e2-4b73-b064-98909ceabbaf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:23.904613Z",
     "iopub.status.busy": "2024-06-09T18:38:23.903783Z",
     "iopub.status.idle": "2024-06-09T18:38:23.912725Z",
     "shell.execute_reply": "2024-06-09T18:38:23.912057Z",
     "shell.execute_reply.started": "2024-06-09T18:38:23.904576Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install pymystem3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7537690a-6850-49c6-aaec-0f3417435b23",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:23.914329Z",
     "iopub.status.busy": "2024-06-09T18:38:23.913580Z",
     "iopub.status.idle": "2024-06-09T18:38:23.923083Z",
     "shell.execute_reply": "2024-06-09T18:38:23.922342Z",
     "shell.execute_reply.started": "2024-06-09T18:38:23.914297Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ec5f3b9-7dc5-4b2d-924d-112baeb13911",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:23.925525Z",
     "iopub.status.busy": "2024-06-09T18:38:23.924331Z",
     "iopub.status.idle": "2024-06-09T18:38:24.111288Z",
     "shell.execute_reply": "2024-06-09T18:38:24.110437Z",
     "shell.execute_reply.started": "2024-06-09T18:38:23.925488Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Бесполезно что-то пытаться донести так до люде...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>А свиньи разве умеют читать?\\n</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Червепидорские страны парашной конфедерации -К...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Нет, это не так. Я зашёл сюда специально за эт...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Дополнение: Дентрен, та чилийская грязь, на ко...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                            comment  toxic\n",
       "0   0  Бесполезно что-то пытаться донести так до люде...      0\n",
       "1   1                     А свиньи разве умеют читать?\\n      1\n",
       "2   2  Червепидорские страны парашной конфедерации -К...      1\n",
       "3   3  Нет, это не так. Я зашёл сюда специально за эт...      1\n",
       "4   4  Дополнение: Дентрен, та чилийская грязь, на ко...      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем наш датасет\n",
    "data_train = pd.read_csv('train.csv.zip', engine='python', sep = ';')\n",
    "data_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb320c6-eaf8-47cb-ac6c-e20b4f446bd3",
   "metadata": {},
   "source": [
    "Сделаем дополнительную аугментацию данных для улучшения результатов модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "43e3dfff-dfee-4c1c-9919-52873e62d651",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:24.113585Z",
     "iopub.status.busy": "2024-06-09T18:38:24.112387Z",
     "iopub.status.idle": "2024-06-09T18:38:24.167379Z",
     "shell.execute_reply": "2024-06-09T18:38:24.166624Z",
     "shell.execute_reply.started": "2024-06-09T18:38:24.113548Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>toxic_comment</th>\n",
       "      <th>neutral_comment1</th>\n",
       "      <th>neutral_comment2</th>\n",
       "      <th>neutral_comment3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>и,чё,блядь где этот херой был до этого со свои...</td>\n",
       "      <td>Ну и где этот герой был,со своими доказательст...</td>\n",
       "      <td>Где этот герой был до этого со своими доказате...</td>\n",
       "      <td>и,где этот герой был до этого со своими доказа...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>О, а есть деанон этого петуха?</td>\n",
       "      <td>О, а есть деанон</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>херну всякую пишут,из-за этого лайка.долбоебизм.</td>\n",
       "      <td>Чушь всякую пишут, из- за этого лайка.</td>\n",
       "      <td>Ерунду всякую пишут,из-за этого лайка.</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>из за таких пидоров мы и страдаем</td>\n",
       "      <td>из за таких плохих людей мы и страдаем</td>\n",
       "      <td>Из-за таких людей мы и страдаем</td>\n",
       "      <td>из за таких как он мы и страдаем</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>гондон путинский он а не артист</td>\n",
       "      <td>Человек Путина он, а не артист</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  ...                                   neutral_comment3\n",
       "0      0  ...  и,где этот герой был до этого со своими доказа...\n",
       "1      1  ...                                                NaN\n",
       "2      2  ...                                                NaN\n",
       "3      3  ...                   из за таких как он мы и страдаем\n",
       "4      4  ...                                                NaN\n",
       "\n",
       "[5 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# загружаем датасет с https://github.com/s-nlp/russe_detox_2022/blob/main/data/input/train.tsv\n",
    "d1 = pd.read_csv('train.tsv', engine='python', sep='\\t')\n",
    "d1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1281a45-340d-4d93-9238-bc32560f877e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:24.169749Z",
     "iopub.status.busy": "2024-06-09T18:38:24.168657Z",
     "iopub.status.idle": "2024-06-09T18:38:25.403941Z",
     "shell.execute_reply": "2024-06-09T18:38:25.402910Z",
     "shell.execute_reply.started": "2024-06-09T18:38:24.169714Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>видимо в разных регионах называют по разному ,...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>понятно что это нарушение правил, писать капсл...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>какие классные, жизненные стихи....</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>а и правда-когда его запретили?...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>в соленой воде вирусы живут .ученые изучали со...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223456</th>\n",
       "      <td>вова - дима когда же вы подавитесь деньгами???...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223457</th>\n",
       "      <td>какая красота, просто нет слов выразить чувств...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223458</th>\n",
       "      <td>вы пост гаи выставити на перекрестке возле 21 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223459</th>\n",
       "      <td>как -то на лебедей непохожи</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223460</th>\n",
       "      <td>интересно чей это самолет!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223461 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0       видимо в разных регионах называют по разному ,...      0\n",
       "1       понятно что это нарушение правил, писать капсл...      1\n",
       "2                     какие классные, жизненные стихи....      0\n",
       "3                      а и правда-когда его запретили?...      0\n",
       "4       в соленой воде вирусы живут .ученые изучали со...      0\n",
       "...                                                   ...    ...\n",
       "223456  вова - дима когда же вы подавитесь деньгами???...      0\n",
       "223457  какая красота, просто нет слов выразить чувств...      0\n",
       "223458  вы пост гаи выставити на перекрестке возле 21 ...      0\n",
       "223459                        как -то на лебедей непохожи      0\n",
       "223460                         интересно чей это самолет!      0\n",
       "\n",
       "[223461 rows x 2 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/AlexSham/Toxic_Russian_Comments\n",
    "\n",
    "file_path = 'train.jsonl'\n",
    "# Чтение файла JSON и преобразование в DataFrame\n",
    "df = pd.read_json(file_path, lines=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5c9e3f55-bfd7-4d5c-ad5f-991c7fd22c48",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:25.405710Z",
     "iopub.status.busy": "2024-06-09T18:38:25.405091Z",
     "iopub.status.idle": "2024-06-09T18:38:25.415546Z",
     "shell.execute_reply": "2024-06-09T18:38:25.414819Z",
     "shell.execute_reply.started": "2024-06-09T18:38:25.405676Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # токсичные комментарии различной тематики https://huggingface.co/datasets/NiGuLa/Russian_Sensitive_Topics\n",
    "# d3 = pd.read_csv('sensitive_topics.csv', engine='python')\n",
    "# d3.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8a7b956e-6b34-4c8a-81c2-b3264c60f952",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:25.417218Z",
     "iopub.status.busy": "2024-06-09T18:38:25.416630Z",
     "iopub.status.idle": "2024-06-09T18:38:25.430362Z",
     "shell.execute_reply": "2024-06-09T18:38:25.429682Z",
     "shell.execute_reply.started": "2024-06-09T18:38:25.417172Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# преобразуем все датасеты чтобы можно было их объединить\n",
    "\n",
    "# d3['toxic']=1\n",
    "# d3['comment'] = d3['text']\n",
    "\n",
    "df_toxic = pd.DataFrame({'comment': d1['toxic_comment'],\n",
    "                         'toxic': [1]*len(d1) })\n",
    "\n",
    "df_non_toxic = pd.DataFrame({'comment': d1['neutral_comment1'] ,\n",
    "                             'toxic': [0]*len(d1)})\n",
    "\n",
    "df.columns = ['comment','toxic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "69bc44f9-f5fd-4bf8-b6c2-39fa00ed84d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:25.432287Z",
     "iopub.status.busy": "2024-06-09T18:38:25.431530Z",
     "iopub.status.idle": "2024-06-09T18:38:25.447743Z",
     "shell.execute_reply": "2024-06-09T18:38:25.447007Z",
     "shell.execute_reply.started": "2024-06-09T18:38:25.432255Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_combined = pd.concat([df_toxic, df_non_toxic, data_train[['comment','toxic']], df])\n",
    "# d3[['comment','toxic']],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "cf508a80-a1b6-4bd5-9bc8-f68243261d62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:25.449524Z",
     "iopub.status.busy": "2024-06-09T18:38:25.448839Z",
     "iopub.status.idle": "2024-06-09T18:38:25.468555Z",
     "shell.execute_reply": "2024-06-09T18:38:25.467904Z",
     "shell.execute_reply.started": "2024-06-09T18:38:25.449477Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>и,чё,блядь где этот херой был до этого со свои...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>О, а есть деанон этого петуха?</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>херну всякую пишут,из-за этого лайка.долбоебизм.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>из за таких пидоров мы и страдаем</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>гондон путинский он а не артист</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223456</th>\n",
       "      <td>вова - дима когда же вы подавитесь деньгами???...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223457</th>\n",
       "      <td>какая красота, просто нет слов выразить чувств...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223458</th>\n",
       "      <td>вы пост гаи выставити на перекрестке возле 21 ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223459</th>\n",
       "      <td>как -то на лебедей непохожи</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223460</th>\n",
       "      <td>интересно чей это самолет!</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>252357 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  comment  toxic\n",
       "0       и,чё,блядь где этот херой был до этого со свои...      1\n",
       "1                          О, а есть деанон этого петуха?      1\n",
       "2        херну всякую пишут,из-за этого лайка.долбоебизм.      1\n",
       "3                       из за таких пидоров мы и страдаем      1\n",
       "4                         гондон путинский он а не артист      1\n",
       "...                                                   ...    ...\n",
       "223456  вова - дима когда же вы подавитесь деньгами???...      0\n",
       "223457  какая красота, просто нет слов выразить чувств...      0\n",
       "223458  вы пост гаи выставити на перекрестке возле 21 ...      0\n",
       "223459                        как -то на лебедей непохожи      0\n",
       "223460                         интересно чей это самолет!      0\n",
       "\n",
       "[252357 rows x 2 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f47b0b4-ca3d-4903-acd6-632b85a03482",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:25.470868Z",
     "iopub.status.busy": "2024-06-09T18:38:25.470022Z",
     "iopub.status.idle": "2024-06-09T18:38:25.677305Z",
     "shell.execute_reply": "2024-06-09T18:38:25.676510Z",
     "shell.execute_reply.started": "2024-06-09T18:38:25.470833Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">comment</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>toxic</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201302</td>\n",
       "      <td>201294</td>\n",
       "      <td>Замолчи</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51055</td>\n",
       "      <td>48803</td>\n",
       "      <td>и,чё,блядь где этот херой был до этого со свои...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      comment                                                                \n",
       "        count  unique                                                top freq\n",
       "toxic                                                                        \n",
       "0      201302  201294                                            Замолчи    2\n",
       "1       51055   48803  и,чё,блядь где этот херой был до этого со свои...    2"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined.groupby('toxic').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5074fac2-79b6-49a3-b9a7-9df6e78fbfa1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:25.679381Z",
     "iopub.status.busy": "2024-06-09T18:38:25.678415Z",
     "iopub.status.idle": "2024-06-09T18:38:25.698939Z",
     "shell.execute_reply": "2024-06-09T18:38:25.698233Z",
     "shell.execute_reply.started": "2024-06-09T18:38:25.679345Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text = np.array(df_combined.comment.values).tolist()\n",
    "target = df_combined.toxic.astype(int).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1b8a43f6-df4c-4fa5-95aa-e4aa5b097775",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:25.700959Z",
     "iopub.status.busy": "2024-06-09T18:38:25.700032Z",
     "iopub.status.idle": "2024-06-09T18:38:25.710156Z",
     "shell.execute_reply": "2024-06-09T18:38:25.709541Z",
     "shell.execute_reply.started": "2024-06-09T18:38:25.700919Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['и,чё,блядь где этот херой был до этого со своими доказательствами?',\n",
       " 'О, а есть деанон этого петуха?',\n",
       " 'херну всякую пишут,из-за этого лайка.долбоебизм.']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0dfed2-c334-4fd3-9d11-4a52d9372562",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Токенизация"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf50913-c10f-4a36-ad85-96a699906f4d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Собственный токенайзер"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "458148f8-79ef-4b3f-b51c-6e61251a416a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:25.711918Z",
     "iopub.status.busy": "2024-06-09T18:38:25.711142Z",
     "iopub.status.idle": "2024-06-09T18:38:25.729637Z",
     "shell.execute_reply": "2024-06-09T18:38:25.728868Z",
     "shell.execute_reply.started": "2024-06-09T18:38:25.711884Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cleanText(string):\n",
    "    \"\"\"This function deletes all symbols except Cyrilic and Base Latin alphabet,\n",
    "    stopwords, functional parts of speech. Returns string of words stem.\"\"\"\n",
    "    # Common cleaning\n",
    "    string = string.lower()\n",
    "    string = re.sub(r\"http\\S+\", \"\", string)\n",
    "    string = re.sub(r'(?:(?!\\u0301)[\\W\\d_])+', ' ', string) # нужно избавиться от нерусских символов\n",
    "    string = re.sub('[^а-яА-Я]+', ' ', string)\n",
    "    string = str.replace(string,'Ё','е')\n",
    "    string = str.replace(string,'ё','е')\n",
    "\n",
    "    # prog = re.compile('[А-Яа-яA-Za-z]+')\n",
    "    # words = prog.findall(string.lower())\n",
    "    tokens = word_tokenize(string)\n",
    "\n",
    "    \n",
    "    # Word Cleaning\n",
    "    ## Stop Words\n",
    "    stopwords = nltk.corpus.stopwords.words('russian')\n",
    "    words = [w for w in tokens if w not in stopwords]\n",
    "    ## Cleaning functional POS (Parts of Speech)\n",
    "    functionalPos = {'CONJ', 'PRCL'}\n",
    "    words = [w for w, pos in nltk.pos_tag(words, lang='rus') if pos not in functionalPos]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = SnowballStemmer('russian')\n",
    "    stemmed_words = [stemmer.stem(w) for w in words]\n",
    "\n",
    "    # lemmatizer = pymorphy3.MorphAnalyzer()\n",
    "#     lemmatizer = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "#     stemmed_words = [lemmatizer.parse(w)[0].normal_form for w in words]\n",
    "\n",
    "#     ЛЕММАТАЙЗЕРЫ работают оооочень долго\n",
    "    # mystem = Mystem()\n",
    "    # stemmed_words = [mystem.lemmatize(w) for w in words]\n",
    "    \n",
    "    # words = [word_tokenize(comment) for comment in words]\n",
    "    # stemmed_words = [word_tokenize(comment) for comment in stemmed_words]\n",
    "\n",
    "    # return ' '.join(stemmed_words)\n",
    "    return stemmed_words\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8003600e-3559-412e-a4fa-b6f34ac652e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:38:25.734807Z",
     "iopub.status.busy": "2024-06-09T18:38:25.733931Z",
     "iopub.status.idle": "2024-06-09T18:44:12.992127Z",
     "shell.execute_reply": "2024-06-09T18:44:12.991392Z",
     "shell.execute_reply.started": "2024-06-09T18:38:25.734768Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_my_tokenization = list(map(cleanText, text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7db157c4-1014-446c-948b-c574dd9d67f4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:44:12.994211Z",
     "iopub.status.busy": "2024-06-09T18:44:12.993653Z",
     "iopub.status.idle": "2024-06-09T18:44:13.004575Z",
     "shell.execute_reply": "2024-06-09T18:44:13.003948Z",
     "shell.execute_reply.started": "2024-06-09T18:44:12.994160Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ч', 'бляд', 'хер', 'сво', 'доказательств'],\n",
       " ['деанон', 'петух'],\n",
       " ['херн', 'всяк', 'пишут', 'лайк', 'долбоебизм']]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_my_tokenization[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f41cf4-953c-48e0-94b3-884766316678",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T11:30:57.782800Z",
     "iopub.status.busy": "2024-06-09T11:30:57.781687Z",
     "iopub.status.idle": "2024-06-09T11:30:57.794095Z",
     "shell.execute_reply": "2024-06-09T11:30:57.793229Z",
     "shell.execute_reply.started": "2024-06-09T11:30:57.782756Z"
    },
    "tags": []
   },
   "source": [
    "## Предобученный токенайзер Rusvectores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f08289ba-d5c6-4da7-af57-dab274bbd0dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:44:13.006518Z",
     "iopub.status.busy": "2024-06-09T18:44:13.005504Z",
     "iopub.status.idle": "2024-06-09T18:44:13.019120Z",
     "shell.execute_reply": "2024-06-09T18:44:13.018498Z",
     "shell.execute_reply.started": "2024-06-09T18:44:13.006483Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install wget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4b13c261-b11e-46c5-b47e-f787e455dcef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:44:13.021196Z",
     "iopub.status.busy": "2024-06-09T18:44:13.020220Z",
     "iopub.status.idle": "2024-06-09T18:44:22.774317Z",
     "shell.execute_reply": "2024-06-09T18:44:22.773536Z",
     "shell.execute_reply.started": "2024-06-09T18:44:13.021158Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import wget\n",
    "model_url = 'http://vectors.nlpl.eu/repository/20/220.zip'\n",
    "m = wget.download(model_url)\n",
    "model_file = '220.zip'\n",
    "# берем модель и распаковываем ее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd960ad6-9386-4a09-89c5-6bf3690cf6ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:44:22.775990Z",
     "iopub.status.busy": "2024-06-09T18:44:22.775011Z",
     "iopub.status.idle": "2024-06-09T18:45:05.066350Z",
     "shell.execute_reply": "2024-06-09T18:45:05.065571Z",
     "shell.execute_reply.started": "2024-06-09T18:44:22.775953Z"
    }
   },
   "outputs": [],
   "source": [
    "with zipfile.ZipFile(model_file, 'r') as archive:\n",
    "    archive.extractall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d60d8992-db58-42d4-8acc-9f1563794774",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:05.068245Z",
     "iopub.status.busy": "2024-06-09T18:45:05.067343Z",
     "iopub.status.idle": "2024-06-09T18:45:08.482357Z",
     "shell.execute_reply": "2024-06-09T18:45:08.481579Z",
     "shell.execute_reply.started": "2024-06-09T18:45:05.068207Z"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "model_rusvec = gensim.models.KeyedVectors.load_word2vec_format('model.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "564ea213-d785-4ee5-8c20-e7e7167f630f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:08.484459Z",
     "iopub.status.busy": "2024-06-09T18:45:08.483499Z",
     "iopub.status.idle": "2024-06-09T18:45:08.495549Z",
     "shell.execute_reply": "2024-06-09T18:45:08.494834Z",
     "shell.execute_reply.started": "2024-06-09T18:45:08.484419Z"
    }
   },
   "outputs": [],
   "source": [
    "def num_replace(word):\n",
    "    newtoken = \"x\" * len(word)\n",
    "    return newtoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f90b92e8-3785-4ba5-8ed7-747a8555a210",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:08.497259Z",
     "iopub.status.busy": "2024-06-09T18:45:08.496522Z",
     "iopub.status.idle": "2024-06-09T18:45:08.523048Z",
     "shell.execute_reply": "2024-06-09T18:45:08.522362Z",
     "shell.execute_reply.started": "2024-06-09T18:45:08.497228Z"
    }
   },
   "outputs": [],
   "source": [
    "def process(pipeline, text='Строка', keep_pos=True, keep_punct=False):\n",
    "    entities = {'PROPN'}\n",
    "    named = False  # переменная для запоминания того, что нам встретилось имя собственное\n",
    "    memory = []\n",
    "    mem_case = None\n",
    "    mem_number = None\n",
    "    tagged_propn = []\n",
    "\n",
    "    # обрабатываем текст, получаем результат в формате conllu:\n",
    "    processed = pipeline.process(text)\n",
    "\n",
    "    # пропускаем строки со служебной информацией:\n",
    "    content = [l for l in processed.split('\\n') if not l.startswith('#')]\n",
    "\n",
    "    # извлекаем из обработанного текста леммы, тэги и морфологические характеристики\n",
    "    tagged = [w.split('\\t') for w in content if w]\n",
    "\n",
    "    for t in tagged:\n",
    "        if len(t) != 10: # если список короткий — строчка не содержит разбора, пропускаем\n",
    "            continue\n",
    "        (word_id,token,lemma,pos,xpos,feats,head,deprel,deps,misc) = t\n",
    "        if not lemma or not token: # если слово пустое — пропускаем\n",
    "            continue\n",
    "            # Разметка имен собственных!!!!!!!!!!!!!!!!!!!!\n",
    "        if pos in entities: # здесь отдельно обрабатываем имена собственные — они требуют особого обращения\n",
    "            if '|' not in feats:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            morph = {el.split('=')[0]: el.split('=')[1] for el in feats.split('|')}\n",
    "            if 'Case' not in morph or 'Number' not in morph:\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "                continue\n",
    "            if not named:\n",
    "                named = True\n",
    "                mem_case = morph['Case']\n",
    "                mem_number = morph['Number']\n",
    "            if morph['Case'] == mem_case and morph['Number'] == mem_number:\n",
    "                memory.append(lemma)\n",
    "                if 'SpacesAfter=\\\\n' in misc or 'SpacesAfter=\\s\\\\n' in misc:\n",
    "                    named = False\n",
    "                    past_lemma = '::'.join(memory)\n",
    "                    memory = []\n",
    "                    tagged_propn.append(past_lemma + '_PROPN ')\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "        else:\n",
    "            # обработка для чисел\n",
    "            if not named:\n",
    "                if pos == 'NUM' and token.isdigit():  # Заменяем числа на xxxxx той же длины\n",
    "                    lemma = num_replace(token)\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "            else:\n",
    "                named = False\n",
    "                past_lemma = '::'.join(memory)\n",
    "                memory = []\n",
    "                tagged_propn.append(past_lemma + '_PROPN ')\n",
    "                tagged_propn.append('%s_%s' % (lemma, pos))\n",
    "\n",
    "    if not keep_punct: # обрабатываем случай, когда не надо сохранять пунктуацию (по умолчанию она сохраняется)\n",
    "        tagged_propn = [word for word in tagged_propn if word.split('_')[1] != 'PUNCT']\n",
    "    if not keep_pos:\n",
    "        tagged_propn = [word.split('_')[0] for word in tagged_propn]\n",
    "    return tagged_propn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b1bf0fb-f5bd-4413-a548-50f37722dad5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:08.524515Z",
     "iopub.status.busy": "2024-06-09T18:45:08.523944Z",
     "iopub.status.idle": "2024-06-09T18:45:08.534945Z",
     "shell.execute_reply": "2024-06-09T18:45:08.534161Z",
     "shell.execute_reply.started": "2024-06-09T18:45:08.524474Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install ufal.udpipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "55bb8a6a-91d1-4b07-bbb3-62ef509f0f6a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:08.536829Z",
     "iopub.status.busy": "2024-06-09T18:45:08.535956Z",
     "iopub.status.idle": "2024-06-09T18:45:08.559116Z",
     "shell.execute_reply": "2024-06-09T18:45:08.558296Z",
     "shell.execute_reply.started": "2024-06-09T18:45:08.536794Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from ufal.udpipe import Model, Pipeline # это пайплайн для обработки модели русвекторс\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "\n",
    "def tag_ud(text='Текст нужно передать функции в виде строки!', modelfile='udpipe_syntagrus.model'):\n",
    "    udpipe_model_url = 'https://rusvectores.org/static/models/udpipe_syntagrus.model'\n",
    "    udpipe_filename = udpipe_model_url.split('/')[-1]\n",
    "\n",
    "    if not os.path.isfile(modelfile):\n",
    "        print('UDPipe model not found. Downloading...', file=sys.stderr)\n",
    "        wget.download(udpipe_model_url)\n",
    "\n",
    "    # print('\\nLoading the model...', file=sys.stderr)\n",
    "    model = Model.load(modelfile)\n",
    "    process_pipeline = Pipeline(model, 'tokenize', Pipeline.DEFAULT, Pipeline.DEFAULT, 'conllu')\n",
    "\n",
    "    # print('Processing input...', file=sys.stderr)\n",
    "    lines = text.split('\\n')\n",
    "    tagged = []\n",
    "    for line in lines:\n",
    "        # line = unify_sym(line.strip()) # здесь могла бы быть ваша функция очистки текста\n",
    "        output = process(process_pipeline, text=line)\n",
    "        tagged_line = ' '.join(output)\n",
    "        tagged.append(tagged_line)\n",
    "    return '\\n'.join(tagged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff25b56d-3af1-4469-b2c3-dcd4b486fc57",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:08.561294Z",
     "iopub.status.busy": "2024-06-09T18:45:08.560167Z",
     "iopub.status.idle": "2024-06-09T18:45:19.031548Z",
     "shell.execute_reply": "2024-06-09T18:45:19.030634Z",
     "shell.execute_reply.started": "2024-06-09T18:45:08.561254Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "text_pretrained_tokenization = [[', '.join(tag_ud(comment).split())] for comment in text[:3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "816f1c65-b183-42a1-9069-8de1384e48da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:19.033807Z",
     "iopub.status.busy": "2024-06-09T18:45:19.032682Z",
     "iopub.status.idle": "2024-06-09T18:45:19.043881Z",
     "shell.execute_reply": "2024-06-09T18:45:19.043224Z",
     "shell.execute_reply.started": "2024-06-09T18:45:19.033754Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['и_CCONJ, ,чё,блядь_ADV, где_ADV, этот_DET, херый_NOUN, быть_AUX, до_ADP, это_PRON, с_ADP, свой_DET, доказательство_NOUN'],\n",
       " ['о_INTJ, а_CCONJ, быть_VERB, деанон_PART, этот_DET, петух_NOUN'],\n",
       " ['херн_NOUN, всякий_DET, писать_VERB, ,из-з_ADP, этот_PRON, лайка.долбоебизм_NOUN']]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pretrained_tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cb5ca8-2fa3-47b4-897d-14c1be0792e1",
   "metadata": {},
   "source": [
    "## Разделение набора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bb0bae86-6349-4d0d-a9c5-3e60df23cf9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:19.045949Z",
     "iopub.status.busy": "2024-06-09T18:45:19.044826Z",
     "iopub.status.idle": "2024-06-09T18:45:19.194610Z",
     "shell.execute_reply": "2024-06-09T18:45:19.193819Z",
     "shell.execute_reply.started": "2024-06-09T18:45:19.045920Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Разделение на тренировочный (96%) и вспомогательный наборы (10%)\n",
    "X_train, X_noo, y_train, y_noo = train_test_split(text_my_tokenization, target, test_size=0.04, random_state=42, stratify=target)\n",
    "\n",
    "# Разделение вспомогательного набора на валидационный (60%) и тестовый наборы (40%)\n",
    "X_valid, X_test, y_valid, y_test = train_test_split(X_noo, y_noo, test_size=0.4, random_state=42, stratify=y_noo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e30c892-d068-4316-a887-24f24e3f144d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Word2vec через словарь и ручное присвоение индексов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e64e1c73-dc57-49eb-9532-c543a818d3b8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:19.196276Z",
     "iopub.status.busy": "2024-06-09T18:45:19.195591Z",
     "iopub.status.idle": "2024-06-09T18:45:19.210472Z",
     "shell.execute_reply": "2024-06-09T18:45:19.209552Z",
     "shell.execute_reply.started": "2024-06-09T18:45:19.196236Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from collections import Counter\n",
    "# vocab = Counter()\n",
    "\n",
    "# for comment in text_my_tokenization:\n",
    "#     vocab.update(comment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "936286bf-5d93-4036-88a5-8442d8a6224d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:19.212768Z",
     "iopub.status.busy": "2024-06-09T18:45:19.211539Z",
     "iopub.status.idle": "2024-06-09T18:45:19.224764Z",
     "shell.execute_reply": "2024-06-09T18:45:19.223817Z",
     "shell.execute_reply.started": "2024-06-09T18:45:19.212729Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a3d46ef5-8e55-4463-8f51-845749c5f308",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:19.226643Z",
     "iopub.status.busy": "2024-06-09T18:45:19.225834Z",
     "iopub.status.idle": "2024-06-09T18:45:19.238956Z",
     "shell.execute_reply": "2024-06-09T18:45:19.238136Z",
     "shell.execute_reply.started": "2024-06-09T18:45:19.226605Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# filtered_vocab = set()\n",
    "# # удаляем слова из словаря которые встречаются реже чем 21 раз\n",
    "# for word in vocab:\n",
    "#     if vocab[word] > 20:\n",
    "#         filtered_vocab.add(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f3957646-c22b-487b-85c5-655475b9f486",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:19.240978Z",
     "iopub.status.busy": "2024-06-09T18:45:19.240048Z",
     "iopub.status.idle": "2024-06-09T18:45:19.250792Z",
     "shell.execute_reply": "2024-06-09T18:45:19.250120Z",
     "shell.execute_reply.started": "2024-06-09T18:45:19.240937Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# len(filtered_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7b7ef2a5-3c37-40c8-a8cc-f52820e64027",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:19.252625Z",
     "iopub.status.busy": "2024-06-09T18:45:19.251718Z",
     "iopub.status.idle": "2024-06-09T18:45:19.264785Z",
     "shell.execute_reply": "2024-06-09T18:45:19.264113Z",
     "shell.execute_reply.started": "2024-06-09T18:45:19.252597Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# word2id = {'PAD':0}\n",
    "# # переименовываем слова в айдишники\n",
    "# for word in filtered_vocab:\n",
    "#     word2id[word] = len(word2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "86b037b9-850d-4a04-966c-c99646e6cdd7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:19.266807Z",
     "iopub.status.busy": "2024-06-09T18:45:19.265764Z",
     "iopub.status.idle": "2024-06-09T18:45:19.283378Z",
     "shell.execute_reply": "2024-06-09T18:45:19.282639Z",
     "shell.execute_reply.started": "2024-06-09T18:45:19.266767Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # обратное преобразование из айди в слово\n",
    "# id2word = {i:word for word, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "06cb1075-3c49-407c-9586-51118db51075",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:19.285211Z",
     "iopub.status.busy": "2024-06-09T18:45:19.284536Z",
     "iopub.status.idle": "2024-06-09T18:45:19.294609Z",
     "shell.execute_reply": "2024-06-09T18:45:19.293972Z",
     "shell.execute_reply.started": "2024-06-09T18:45:19.285180Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sentences = [] # комментарий в наших данных\n",
    "# documents = []\n",
    "\n",
    "# # преобразуем статьи\n",
    "# for comment in text_my_tokenizationt:\n",
    "#     tokens = comment\n",
    "#     if not tokens:\n",
    "#         continue\n",
    "#     documents.append(tokens)\n",
    "#     ids = [word2id[token] for token in tokens if token in word2id]\n",
    "#     sentences.append(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "07f9c66b-6d7b-4bbb-912f-c0237c34417f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:19.296310Z",
     "iopub.status.busy": "2024-06-09T18:45:19.295521Z",
     "iopub.status.idle": "2024-06-09T18:45:19.304834Z",
     "shell.execute_reply": "2024-06-09T18:45:19.304080Z",
     "shell.execute_reply.started": "2024-06-09T18:45:19.296271Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #cbow\n",
    "# X = []\n",
    "# y = []\n",
    "\n",
    "# window = 10\n",
    "# for sent in sentences:\n",
    "#     for i in range(len(sent)-1):\n",
    "#         word = sent[i]\n",
    "#         context = sent[max(0, i-window):i] + sent[i+1:i+window]\n",
    "\n",
    "#         X.append(context)\n",
    "#         y.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bae0274b-4c41-4475-9430-ce49f7894f44",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:19.306803Z",
     "iopub.status.busy": "2024-06-09T18:45:19.306135Z",
     "iopub.status.idle": "2024-06-09T18:45:19.317071Z",
     "shell.execute_reply": "2024-06-09T18:45:19.316386Z",
     "shell.execute_reply.started": "2024-06-09T18:45:19.306769Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# # иксы могут быть разного размера поэтому надо обрезать, либо дополнить недостающую часть\n",
    "# X_train = tf.keras.preprocessing.sequence.pad_sequences(X_train, maxlen=20, padding='post')\n",
    "# y_train = np.array(y_train)\n",
    "# X_valid = tf.keras.preprocessing.sequence.pad_sequences(X_valid, maxlen=20, padding='post')\n",
    "# y_valid = np.array(y_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "40a39ae9-e3b4-4f49-a943-8b511031210a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:19.318932Z",
     "iopub.status.busy": "2024-06-09T18:45:19.317988Z",
     "iopub.status.idle": "2024-06-09T18:45:19.326619Z",
     "shell.execute_reply": "2024-06-09T18:45:19.325967Z",
     "shell.execute_reply.started": "2024-06-09T18:45:19.318897Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "63668e22-62f6-4e1c-b93b-eda748a31190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:19.328506Z",
     "iopub.status.busy": "2024-06-09T18:45:19.327752Z",
     "iopub.status.idle": "2024-06-09T18:45:19.336585Z",
     "shell.execute_reply": "2024-06-09T18:45:19.335966Z",
     "shell.execute_reply.started": "2024-06-09T18:45:19.328468Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_cbow = tf.keras.Sequential()\n",
    "\n",
    "# model_cbow.add(tf.keras.layers.Embedding(input_dim=len(word2id),\n",
    "#                                     input_length=20,\n",
    "#                                     output_dim=30))\n",
    "\n",
    "# model_cbow.add(tf.keras.layers.Lambda(lambda x: tf.keras.backend.sum(x, axis=1))) # суммируем по 2й размерности\n",
    "\n",
    "# model_cbow.add(tf.keras.layers.Dense(len(word2id), activation='softmax'))\n",
    "# model_cbow.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='sparse_categorical_crossentropy'\n",
    "#               )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2661635e-e29c-4413-b817-5876c45170cb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:45:19.337885Z",
     "iopub.status.busy": "2024-06-09T18:45:19.337405Z",
     "iopub.status.idle": "2024-06-09T18:45:19.346671Z",
     "shell.execute_reply": "2024-06-09T18:45:19.346047Z",
     "shell.execute_reply.started": "2024-06-09T18:45:19.337855Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model_cbow.fit(X_train, y_train,\n",
    "#           validation_data=(X_valid, y_valid),\n",
    "#           batch_size=1024,\n",
    "#          epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033744fd-6af6-444e-b6bd-8fadf714ef25",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Word2vec в Gensim + собственная нейросеть"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4e8c0309-21da-4736-b8fb-e00fcfaff3aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T19:03:53.844214Z",
     "iopub.status.busy": "2024-06-09T19:03:53.842984Z",
     "iopub.status.idle": "2024-06-09T19:04:19.778743Z",
     "shell.execute_reply": "2024-06-09T19:04:19.777885Z",
     "shell.execute_reply.started": "2024-06-09T19:03:53.844170Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import gensim\n",
    "\n",
    "word2vec_model = gensim.models.Word2Vec(text_my_tokenization, vector_size=50, window=10, workers=4, sg = 0)\n",
    "word2vec_model.train(text_my_tokenization, total_examples=len(text), epochs=10)\n",
    "\n",
    "class CommentsDataset(Dataset):\n",
    "    def __init__(self, comments, labels, word2vec_model, max_words=25, max_vec_length=50):\n",
    "        self.comments = comments\n",
    "        self.labels = labels\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.input_size = word2vec_model.vector_size\n",
    "        self.max_words = max_words\n",
    "        self.max_vec_length = max_vec_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        comment = self.comments[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        comment_vec = [self.word2vec_model.wv[word] if word in self.word2vec_model.wv else np.zeros(self.input_size, dtype=np.float32) for word in comment[:self.max_words]]\n",
    "\n",
    "        # Padding для длины вектора слов\n",
    "        padded_comment_vec = []\n",
    "        for vec in comment_vec:\n",
    "            if len(vec) < self.max_vec_length:\n",
    "                padded_vec = np.pad(vec, (0, self.max_vec_length - len(vec)), mode='constant')\n",
    "            else:\n",
    "                padded_vec = vec[:self.max_vec_length]\n",
    "            padded_comment_vec.append(padded_vec)\n",
    "\n",
    "        while len(padded_comment_vec) < self.max_words:\n",
    "            padded_comment_vec.append(np.zeros(self.max_vec_length, dtype=np.float32))\n",
    "\n",
    "        return torch.tensor(padded_comment_vec), torch.tensor([label])\n",
    "\n",
    "\n",
    "# Создание DataLoader для тренировочных данных и вадидационных данных\n",
    "train_dataset = CommentsDataset(X_train, y_train, word2vec_model)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "valid_dataset = CommentsDataset(X_valid, y_valid, word2vec_model)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "test_dataset = CommentsDataset(X_test, y_test, word2vec_model)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b92ba6d3-afe8-42b5-87ee-e8e2740427e5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T19:04:19.781493Z",
     "iopub.status.busy": "2024-06-09T19:04:19.780342Z",
     "iopub.status.idle": "2024-06-09T19:04:19.799538Z",
     "shell.execute_reply": "2024-06-09T19:04:19.798860Z",
     "shell.execute_reply.started": "2024-06-09T19:04:19.781455Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Нейронная сеть для классификации\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=50, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        # self.lstm2 = nn.LSTM(64, 32, batch_first=True)\n",
    "        self.fc1 = nn.Linear(64, 32)\n",
    "        self.fc = nn.Linear(32, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm1(x)\n",
    "        # out, _ = self.lstm2(out)        \n",
    "        # out = self.fc(out[:, -1, :])  # Используем только последний выход\n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        out = self.fc(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "# Инициализация модели, функции потерь и оптимизатора\n",
    "model = LSTMModel()\n",
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.BCEWithLogitsLoss() \n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "# ШЕДУЛЕР\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=4, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a0b2e9e6-a01b-4f46-9b6a-e82459399e0d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T19:04:19.801904Z",
     "iopub.status.busy": "2024-06-09T19:04:19.800602Z",
     "iopub.status.idle": "2024-06-09T19:19:25.861231Z",
     "shell.execute_reply": "2024-06-09T19:19:25.860411Z",
     "shell.execute_reply.started": "2024-06-09T19:04:19.801875Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Эпоха 1, Тренировочная ошибка: 0.04880428247198599, Валидационная ошибка: 0.04229793279553692\n",
      "Эпоха 1, Тренировочная точность: 0.9388430707250828, Валидационная точность: 0.9438666006273733\n",
      "Валидационная ошибка уменьшилась\n",
      "Эпоха 2, Тренировочная ошибка: 0.042405700143792475, Валидационная ошибка: 0.04085199668756241\n",
      "Эпоха 2, Тренировочная точность: 0.9423950103606838, Валидационная точность: 0.9459303285454845\n",
      "Валидационная ошибка уменьшилась\n",
      "Эпоха 3, Тренировочная ошибка: 0.04147371333991551, Валидационная ошибка: 0.0423812728921966\n",
      "Эпоха 3, Тренировочная точность: 0.9440770735814944, Валидационная точность: 0.9457377139397942\n",
      "Эпоха 4, Тренировочная ошибка: 0.04165405676266087, Валидационная ошибка: 0.0402006562510147\n",
      "Эпоха 4, Тренировочная точность: 0.944923264895031, Валидационная точность: 0.9463017995707446\n",
      "Валидационная ошибка уменьшилась\n",
      "Эпоха 5, Тренировочная ошибка: 0.03855858711500841, Валидационная ошибка: 0.038714110226100856\n",
      "Эпоха 5, Тренировочная точность: 0.9462614854991703, Валидационная точность: 0.9474987617632491\n",
      "Валидационная ошибка уменьшилась\n",
      "Эпоха 6, Тренировочная ошибка: 0.0366132363620191, Валидационная ошибка: 0.038972700917587456\n",
      "Эпоха 6, Тренировочная точность: 0.9476744186046512, Валидационная точность: 0.9480766055803203\n",
      "Эпоха 7, Тренировочная ошибка: 0.03538314823034341, Валидационная ошибка: 0.03895997108954033\n",
      "Эпоха 7, Тренировочная точность: 0.9489419365338825, Валидационная точность: 0.9485836930116276\n",
      "Эпоха 8, Тренировочная ошибка: 0.03438829807493189, Валидационная ошибка: 0.03836091282730284\n",
      "Эпоха 8, Тренировочная точность: 0.9500494299559981, Валидационная точность: 0.9489846458642892\n",
      "Валидационная ошибка уменьшилась\n",
      "Эпоха 9, Тренировочная ошибка: 0.03305978078921183, Валидационная ошибка: 0.038554308398455917\n",
      "Эпоха 9, Тренировочная точность: 0.9511763664499133, Валидационная точность: 0.9492964980830261\n",
      "Эпоха 10, Тренировочная ошибка: 0.032704486318995536, Валидационная ошибка: 0.038589387566567115\n",
      "Эпоха 10, Тренировочная точность: 0.9521319893338617, Валидационная точность: 0.9495459798580155\n"
     ]
    }
   ],
   "source": [
    "# Обучение модели\n",
    "num_epochs = 10\n",
    "valid_loss_min = np.Inf\n",
    "correct_train = 0\n",
    "total_train = 0\n",
    "correct_valid = 0\n",
    "total_valid = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    model.train()\n",
    "    for comment, label in train_loader:\n",
    "        \n",
    "        # comments = comment.to(device)\n",
    "        comment = comment.to(torch.float32)\n",
    "        # label = label.long()\n",
    "        label = label.float()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(comment)\n",
    "        # outputs = (outputs > 0.5).float()\n",
    "        loss = criterion(outputs, label)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        predicted = (outputs > 0.49).float()  # Применяем порог для бинарной классификации\n",
    "        correct_train += (predicted == label).sum().item()\n",
    "        total_train += label.size(0)\n",
    "        train_loss += loss.item() * comment.size(0)\n",
    "    \n",
    "    model.eval()\n",
    "    for comment, label in valid_loader:\n",
    "\n",
    "        # comment = comment.to(device)\n",
    "        comment = comment.to(torch.float32)\n",
    "        # label = torch.unsqueeze(label, 1).float()\n",
    "        # label = label.long()\n",
    "        label = label.float()\n",
    "        \n",
    "        outputs = model(comment)\n",
    "        # outputs = (outputs > 0.5).float()\n",
    "        \n",
    "        predicted = (outputs > 0.49).float()  # Применяем порог для бинарной классификации\n",
    "        correct_valid += (predicted == label).sum().item()\n",
    "        total_valid += label.size(0)\n",
    "        loss = criterion(outputs, label)\n",
    "        valid_loss += loss.item() * comment.size(0)\n",
    "    \n",
    "    train_loss = train_loss / len(train_loader.dataset)\n",
    "    valid_loss = valid_loss / len(valid_loader.dataset)\n",
    "    train_accuracy = correct_train / total_train\n",
    "    valid_accuracy = correct_valid / total_valid\n",
    "\n",
    "    \n",
    "    print(f'Эпоха {epoch+1}, Тренировочная ошибка: {train_loss}, Валидационная ошибка: {valid_loss}')\n",
    "    print(f'Эпоха {epoch+1}, Тренировочная точность: {train_accuracy}, Валидационная точность: {valid_accuracy}')\n",
    "\n",
    "    # СОХРАНЕНИЕ ЛУЧШЕЙ МОДЕЛИ\n",
    "    if valid_loss < valid_loss_min:\n",
    "        print('Валидационная ошибка уменьшилась')\n",
    "        torch.save(model.state_dict(), 'lstm_model.pth')\n",
    "        valid_loss_min = valid_loss\n",
    "\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e00bca7f-1e10-44a1-9839-a54faeab4c4a",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Классификация трансфер лернингом, используя Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8d25cebc-d24d-4f19-bfb3-f0a444845714",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:58:14.310009Z",
     "iopub.status.busy": "2024-06-09T18:58:14.309046Z",
     "iopub.status.idle": "2024-06-09T18:58:18.783012Z",
     "shell.execute_reply": "2024-06-09T18:58:18.782085Z",
     "shell.execute_reply.started": "2024-06-09T18:58:14.309975Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jupyter/.local/lib/python3.10/site-packages/transformers/utils/hub.py:124: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# load tokenizer and model weights\n",
    "tokenizer = BertTokenizer.from_pretrained('SkolkovoInstitute/russian_toxicity_classifier')\n",
    "model = BertForSequenceClassification.from_pretrained('SkolkovoInstitute/russian_toxicity_classifier')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f2626666-2b72-41a2-989c-b4f99a6661a7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:58:18.784925Z",
     "iopub.status.busy": "2024-06-09T18:58:18.784356Z",
     "iopub.status.idle": "2024-06-09T18:59:43.870856Z",
     "shell.execute_reply": "2024-06-09T18:59:43.870076Z",
     "shell.execute_reply.started": "2024-06-09T18:58:18.784891Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (696 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "encoded_texts = [tokenizer.encode(comment, return_tensors='pt') for comment in text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "73387cfb-68ab-43f9-8443-90ec61ac97ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:59:43.872872Z",
     "iopub.status.busy": "2024-06-09T18:59:43.871913Z",
     "iopub.status.idle": "2024-06-09T18:59:47.146402Z",
     "shell.execute_reply": "2024-06-09T18:59:47.145550Z",
     "shell.execute_reply.started": "2024-06-09T18:59:43.872833Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_size = 25\n",
    "padded_tensors = []\n",
    "\n",
    "for tensor in encoded_texts:\n",
    "    if tensor.size(1) > target_size:  # Обрезаем слишком большие тензоры\n",
    "        new_tensor = tensor[:, :target_size]\n",
    "    else:  # Дополняем нулями слишком маленькие тензоры\n",
    "        new_tensor = torch.cat((tensor, torch.zeros(1, target_size - tensor.size(1))), dim=1)\n",
    "    \n",
    "    padded_tensors.append(new_tensor)\n",
    "\n",
    "# Теперь все тензоры в списке comment_tensors имеют одинаковый размер 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "3891053c-068b-45ce-9be2-578d616de375",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T19:02:29.930219Z",
     "iopub.status.busy": "2024-06-09T19:02:29.929124Z",
     "iopub.status.idle": "2024-06-09T19:02:29.945342Z",
     "shell.execute_reply": "2024-06-09T19:02:29.944673Z",
     "shell.execute_reply.started": "2024-06-09T19:02:29.930154Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CommentsDataset11(Dataset):\n",
    "    def __init__(self, comments, labels):\n",
    "        self.comments = comments\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.comments)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.comments[idx], self.labels[idx]\n",
    "\n",
    "    from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "# Создаем датасет\n",
    "train_dataset = CommentsDataset11(X_train, y_train)\n",
    "# Загрузка данных с помощью даталоадера\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "valid_dataset = CommentsDataset11(X_valid, y_valid)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb901a34-ab4f-4a71-be7a-a083717ecba9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Модель работает очень долго на аугментированных данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee7c13e-0498-4f32-92fd-6b9343aa779f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Предсказание тестовых комментариев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1bdd417f-978d-4767-8c59-5c818f823e74",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:59:47.164931Z",
     "iopub.status.busy": "2024-06-09T18:59:47.164228Z",
     "iopub.status.idle": "2024-06-09T18:59:47.211775Z",
     "shell.execute_reply": "2024-06-09T18:59:47.210909Z",
     "shell.execute_reply.started": "2024-06-09T18:59:47.164896Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMModel(\n",
       "  (lstm1): LSTM(50, 64, batch_first=True)\n",
       "  (fc1): Linear(in_features=64, out_features=32, bias=True)\n",
       "  (fc): Linear(in_features=32, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# выгружаем лучшую модель\n",
    "model = LSTMModel()\n",
    "model.load_state_dict(torch.load('lstm_model.pth'))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "30130867-6cde-48d0-9b62-eb40eeead9ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T18:59:47.213872Z",
     "iopub.status.busy": "2024-06-09T18:59:47.212871Z",
     "iopub.status.idle": "2024-06-09T19:00:03.709205Z",
     "shell.execute_reply": "2024-06-09T19:00:03.708397Z",
     "shell.execute_reply.started": "2024-06-09T18:59:47.213828Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('test.csv.zip', engine='python', sep = ';')\n",
    "data_test.head()\n",
    "test = np.array(data_test.comment.values)\n",
    "id_test = data_test.id\n",
    "test = list(map(cleanText, test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b57c4faf-5e06-4f1c-8588-a29631776399",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T19:02:05.428403Z",
     "iopub.status.busy": "2024-06-09T19:02:05.427735Z",
     "iopub.status.idle": "2024-06-09T19:02:05.439801Z",
     "shell.execute_reply": "2024-06-09T19:02:05.439116Z",
     "shell.execute_reply.started": "2024-06-09T19:02:05.428367Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_of_zeros = [0] * len(test)\n",
    "test_dataset = CommentsDataset(test, list_of_zeros, word2vec_model)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "8c4df345-e75b-4628-9aed-dff7b5302ab6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T19:02:05.441498Z",
     "iopub.status.busy": "2024-06-09T19:02:05.440790Z",
     "iopub.status.idle": "2024-06-09T19:02:06.865226Z",
     "shell.execute_reply": "2024-06-09T19:02:06.864467Z",
     "shell.execute_reply.started": "2024-06-09T19:02:05.441465Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_classes=[]\n",
    "for comment, _ in test_loader:\n",
    "\n",
    "    comment = comment.to(torch.float32)\n",
    "        \n",
    "    outputs = model(comment)\n",
    "        \n",
    "    predicted = (outputs > 0.49).float()  # Применяем порог для бинарной классификации\n",
    "\n",
    "    predicted = predicted.cpu().detach().numpy().squeeze()\n",
    "    predicted_classes.append(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "43d0bef0-6df3-4460-a666-ef51234d7e59",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-09T19:02:06.867531Z",
     "iopub.status.busy": "2024-06-09T19:02:06.866764Z",
     "iopub.status.idle": "2024-06-09T19:02:06.891282Z",
     "shell.execute_reply": "2024-06-09T19:02:06.890420Z",
     "shell.execute_reply.started": "2024-06-09T19:02:06.867494Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "predicted_classes\n",
    "flat_list = list(itertools.chain(*predicted_classes))\n",
    "sub = pd.DataFrame({\"ID\": id_test, \"prediction\": list(map(int, flat_list))})\n",
    "sub.to_csv('sample_submission.csv',index=False)\n",
    "# файл скачается в ту же папку, где находится программный код"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bfd09f-1fc0-40af-86ad-fa6a8ab329a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0861bea9-d22a-4ee3-b6a0-a4d2c12a47ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d08a9e-c1fc-4566-a0d7-d35e1872a554",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4bae22-640e-4b7a-9068-4cce3d452760",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e36699d-ba7a-4f66-8c56-10196768e04f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
